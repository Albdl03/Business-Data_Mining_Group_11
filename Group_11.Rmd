---
title: "BUSINESS DATA MINING GROUP PROJECT"
author: "Alberto de Leo"
date: "2024-10-13"
output:
  pdf_document:
    latex_engine: xelatex
editor_options:
  markdown:
    wrap: sentence
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Importing the necessary libraries

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ROSE) #For unbalanced data
library(caret)
library(skimr) 
library(randomForest) 
library(corrplot)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(FNN)
library(class)
library(neuralnet)
library(nnet)
library(dummies)
```
# Data pre processing


# Load the data

```{r data load, echo=FALSE}
employee_df <- read.csv("./IBM-HR-Employee-Attrition.csv", stringsAsFactors = T)
```

# Data Cleaning and Reduction

Remove columns that are not useful for the analysis
```{r}
employee_df <- employee_df %>% select(-c(Over18, EmployeeCount, StandardHours, EmployeeNumber))
```

```{r}
head(employee_df, 10)
```


```{r}
colSums(is.na(employee_df)) #Check for NA values in the dataset
```
```{r}
str(employee_df) # display the structure of the dataframe
```

```{r}
dim(employee_df) # find the dimension of data frame
```

```{r}
anyDuplicated(employee_df) #check duplicate row
```

```{r}
skim(employee_df) #watch a complete summary of data
```
# Outliers Detection

RECOGNIZE ROWS THAT CONTAIN VALUES THAT ARE OUTLIERS FOR MORE THEN ONE COLUMN

```{r}
par(mfrow = c(3, 7), mar = c(2, 2, 2, 2))

outlier_row = c()
for(i in 1:ncol(employee_df[,-c(2)])){
  if (is.factor(employee_df[,i]) == FALSE){
    
    boxplot(employee_df[,i], col = rgb(.7,.7,.7), main = names(employee_df)[i], horizontal = FALSE)
    
    quartiles = quantile(employee_df[,i], probs=c(.25, .75), na.rm = TRUE)
    
    IQR = IQR(employee_df[,i])
    
    Lower = quartiles[1] - 2*IQR 
    Upper = quartiles[2] + 2*IQR 
    
    counter = 0
    for(row in employee_df[,i]){
      counter = counter + 1
      if (row < Lower || row > Upper){
        outlier_row = append(outlier_row, counter)
      }}}}

outlier_row <- outlier_row[duplicated(outlier_row)]
employee_out = employee_df[-c(outlier_row),] #dataset without outliers

#number of outlier detected from 
#length(outlier_row) 
```

```{r}
# Select numeric columns only for correlation
numeric_columns <- sapply(employee_out, is.numeric)
df_numeric <- employee_out[, numeric_columns]

# Calculate the correlation matrix
cor_matrix <- cor(df_numeric, use = "complete.obs")

# Plot the correlation matrix using corrplot
corrplot(cor_matrix, method = "number", 
         tl.col = "black", tl.srt = 90, # Text label color and rotation,
         number.cex = 0.4, # Text size
         title = "Correlation Matrix of Employee Features", 
         mar=c(0,0,1,0)) # Adjust margins for title
```

# Data Exploration

```{r}
table(employee_out$Attrition)
```
From this graph we see that it is unbalanced. We have more employees who did not leave the company than those who did. This is important to keep in mind when building predictive models, as the class imbalance can affect the model's performance.

```{r}
# Calculate the frequency of Attrition
attrition_counts <- table(employee_out$Attrition)

# Create a pie chart
pie(attrition_counts, labels = paste(names(attrition_counts), round(attrition_counts/sum(attrition_counts)*100, 1), "%"),
    col = c("lightblue", "lightcoral"), main = "Distribution of Attrition")
```
```{r}
ggplot(employee_out, aes(x = BusinessTravel)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Travel Frequency Count", x = "Frequency of Travel", y = "Count")
```

Bar plot for WorkLifeBalance distribution by OverTime

```{r}
ggplot(employee_out, aes(x = factor(WorkLifeBalance), fill = OverTime)) +
  geom_bar(position = "dodge") +
  labs(title = "WorkLifeBalance Distribution by OverTime", x = "WorkLife Balance", y = "Count") +
  scale_fill_manual(values = c("lightblue", "lightcoral"))

```
# Sampling technique
Random Over-Sampling Examples (ROSE) is a method to handle imbalanced data by generating synthetic samples for the minority class. This can help balance the classes and improve the performance of predictive models.

```{r}
#Balancing Data

# Apply ROSE to balance the dataset
balanced_data <- ROSE(Attrition ~ ., data = employee_out, seed = 123)$data

# Check the balance of the target variable
table(balanced_data$Attrition)

```



```{r}
# Split data into training and test sets
set.seed(3)
# Now Selecting 70% of data as sample from total 'n' rows of the data  
sample <- sample.int(n = nrow(balanced_data), size = floor(.7*nrow(balanced_data)), replace = F)
trainData <- balanced_data[sample, ] #we select the sample randomly
testData  <- balanced_data[-sample, ] #we select the remaining as validation data

```

# Model Building

*   Decision Tree
*   Random Forest
*   Logistic Regression

```{r}
set.seed(3)
# Decision Tree model
decision_tree_model <- rpart(Attrition ~ ., data = trainData, method = "class", maxdepth = 8, cp = 0.001, model = TRUE)

#unpruned tree
prp(decision_tree_model,
    type = 2,
    extra = 104,
    under = TRUE,
    split.font = 2,
    varlen = -10,
    main = "Decision Tree for Employee Attrition")

#pruned tree
pfit<- prune(decision_tree_model, 
             cp = decision_tree_model$cptable[which.min(decision_tree_model$cptable[,"xerror"]),"CP"])

prp(pfit, box.palette = c("lightpink", "lightgreen"), type = 1, extra = 1, varlen = -10)


decision_tree_model$variable.importance
decision_tree_model$cptable
```

```{r}
# Predict the test data using the decision tree model
# Get predicted probabilities instead of class labels
y_pred_decision_tree_prob <- predict(decision_tree_model, testData, type = "prob")[,2]  # Probability of the "Yes" class

# Make predictions based on a threshold (default is 0.5)
threshold <- 0.5
y_pred_class <- ifelse(y_pred_decision_tree_prob >= threshold, "Yes", "No")

# Convert the predicted classes into a factor with the same levels as the original Attrition column
y_pred_class <- factor(y_pred_class, levels = c("No", "Yes"))

# Create a confusion matrix to evaluate the performance of the model
confusion_matrix <- table(Predicted = y_pred_class, Actual = testData$Attrition)
confusionMatrix(y_pred_class, testData$Attrition)
```

```{r}
set.seed(3)

random_forest_model <- randomForest(Attrition ~ ., data = trainData, ntree = 100, importance = TRUE)

## variable importance plot
varImpPlot(random_forest_model, type = 1)

# Predict the probabilities for the test set
y_pred_rf_prob <- predict(random_forest_model, testData, type = "prob")

# Extract the predicted probabilities for the "Yes" class (positive class)
y_pred_yes_prob_rf <- y_pred_rf_prob[, 2] # Probability of the "Yes" class

# Make predictions based on a threshold (default is 0.5)
threshold <- 0.5
y_pred_class <- ifelse(y_pred_yes_prob_rf >= threshold, "Yes", "No")

# Convert the predicted classes into a factor with the same levels as the original Attrition column
y_pred_class <- factor(y_pred_class, levels = c("No", "Yes"))
# Evaluate the model
confusionMatrix(y_pred_class, testData$Attrition)

```


#Logistic Regression

```{r}
mod = glm(Attrition~ BusinessTravel +
EnvironmentSatisfaction +
Gender +
HourlyRate +
JobInvolvement +
JobRole +
MaritalStatus +
OverTime +
RelationshipSatisfaction +
WorkLifeBalance +
YearsInCurrentRole +
YearsSinceLastPromotion, 
          family = binomial(link = "logit"),
          data = trainData)
summary(mod)
```


```{r}
# Predict probabilities first
predicted_probabilities <- predict(mod, newdata = testData, type = "response")

# Convert probabilities to class labels (0 or 1)
predicted_classes <- ifelse(predicted_probabilities > 0.5, "Yes", "No")
predicted_classes <- factor(predicted_classes, levels = levels(testData$Attrition))
# Load the caret library
library(caret)

# Create a confusion matrix
confusionMatrix(predicted_classes, testData$Attrition)
```


#ROC and AUC
```{r}
# Load the pROC library
library(pROC)

# Create a ROC curve

#Decision Tree
roc_dt <- pROC::roc(testData$Attrition,
                     y_pred_decision_tree_prob,
                     plot = TRUE,
                     col = "midnightblue",
                     lwd = 3,
                     auc.polygon = T,
                     auc.polygon.col = "lightblue",
                     print.auc = T)

#Random Forest
roc_rf <- pROC::roc(testData$Attrition,
                     y_pred_yes_prob_rf,
                     plot = TRUE,
                     col = "midnightblue",
                     lwd = 3,
                     auc.polygon = T,
                     auc.polygon.col = "lightblue",
                     print.auc = T)


#Logistic Regression
roc_lr <- pROC::roc(testData$Attrition,
                     predicted_probabilities,
                     plot = TRUE,
                     col = "midnightblue",
                     lwd = 3,
                     auc.polygon = T,
                     auc.polygon.col = "lightblue",
                     print.auc = T)
```
# Predict worklifeBalance
```{r}
selected_data <- c("YearsSinceLastPromotion", "YearsInCurrentRole", "YearsAtCompany", "RelationshipSatisfaction", "MonthlyIncome", "JobSatisfaction", "JobLevel", "JobInvolvement", "EnvironmentSatisfaction", "Education", "DistanceFromHome", "DailyRate", "BusinessTravel", "Gender", "MaritalStatus", "OverTime", "Attrition", "WorkLifeBalance")

```

# Regression of WorkLifeBalance

```{r}
set.seed(123)
sample <- createDataPartition(employee_out$WorkLifeBalance, p = 0.7, list = FALSE)
trainData_full <- employee_out[sample, selected_data]  # Full training data (70%)
testData  <- employee_out[-sample, selected_data]      # Test data (30%)

# Step 2: Further split the 70% training data into training (60%) and validation (40% of 70%)
trainIndex <- createDataPartition(trainData_full$WorkLifeBalance, p = 0.6, list = FALSE)
trainData <- trainData_full[trainIndex, ]  # Actual training data (60% of total)
validationData <- trainData_full[-trainIndex, ]  # Validation data (20% of total)
```


# Scaling the data 

```{r}
x_train <- trainData[, -which(names(trainData) == "WorkLifeBalance")]  # All columns except 'WorkLifeBalance'
y_train <- trainData$WorkLifeBalance  # 'WorkLifeBalance' column

x_val <- validationData[, -which(names(validationData) == "WorkLifeBalance")]  # All columns except 'WorkLifeBalance'
y_val <- validationData$WorkLifeBalance  # 'WorkLifeBalance' column

x_test <- testData[, -which(names(testData) == "WorkLifeBalance")]  # All columns except 'WorkLifeBalance'
y_test <- testData$WorkLifeBalance  # 'WorkLifeBalance' column


preProcValues <- preProcess(x_train, method = c("center", "scale"))
x_train_norm <- predict(preProcValues, x_train)
x_val_norm <- predict(preProcValues, x_val)
x_test_norm <- predict(preProcValues, x_test)

```

```{r warning=FALSE}
# Identify categorical columns
categorical_cols <- c("Attrition", "BusinessTravel", "Department", "EducationField", 
                      "Gender", "JobRole", "MaritalStatus", "OverTime")

# Apply one-hot encoding to categorical variables
x_train_norm <- dummy.data.frame(x_train_norm, names = categorical_cols)
x_val_norm <- dummy.data.frame(x_val_norm, names = categorical_cols)
x_test_norm <- dummy.data.frame(x_test_norm, names = categorical_cols)
```

#Knn
Elbow Method: Plot the error rate or accuracy against various k values and identify the point of diminishing returns, often referred to as the “elbow.” This can help pinpoint a suitable k value.

```{r}
# Calculate the RMSE for the validation set
rmse_value <- c()

#knn classification model used
for(i in 1:20) {
  
  knn_pred <- knn.reg(train = x_train_norm, test = x_val_norm, y = y_train, k = i)$pred
  
  # Calculate RMSE for the set
  error = y_val - knn_pred
  rmse_value[i] <- sqrt(mean(error^2))
  cat("The RMSE for the validation set with", i ,"neighbours is:", rmse_value[i], "\n")
}

#plot the RMSE values to find the optimal k
plot(rmse_value, type="b", xlab="K- Value",ylab="RMSE")

```
From the graph, we can see that the RMSE value decreases as the number of neighbors increases. The optimal k value is the point where the RMSE value starts to stabilize or increase. In this case, the optimal k value is 10.

```{r}
#Make prediction using knn with the optimal k value
knn_pred <- knn.reg(train = x_train_norm, test = x_test_norm, y = y_train, k = 10)$pred

#Calculate evaluation metrics
error = y_test - knn_pred
rmse = sqrt(mean(error^2))
cat("The RMSE for the test set is:", rmse, "\n")

```
# Neural network for regression problem

```{r}
set.seed(123)

# Create a neural network model with two hidden layers with 3 nodes each
nn_model <- neuralnet(y_train ~ ., data = data.frame(x_train_norm, y_train), hidden = 3, linear.output = TRUE)

# Ensure that column names of validation and test data are consistent with training data
colnames(x_val_norm) <- colnames(x_train_norm)
colnames(x_test_norm) <- colnames(x_train_norm)

# Predict using validation data
validation_predictions <- compute(nn_model, x_val_norm)$net.result

# Manually calculate RMSE for validation set
validation_rmse <- sqrt(mean((y_val - validation_predictions)^2))

# Predict using test data
test_predictions <- compute(nn_model, x_test_norm)$net.result

# Manually calculate RMSE for test set
test_rmse <- sqrt(mean((y_test - test_predictions)^2))

# Print RMSE values
cat("Validation RMSE for 1 hidden layers with 4 nodes:", validation_rmse, "\n")
cat("Test RMSE for 1 hidden layers with 4 nodes each:", test_rmse, "\n")

# Plot the neural network
plot(nn_model, rep = "best")
```


```{r}
set.seed(123)

# Create a neural network model with two hidden layers with 4 nodes each
nn_model <- neuralnet(y_train ~ ., data = data.frame(x_train_norm, y_train), hidden = c(3, 3), linear.output = TRUE)

# Predict using validation data
validation_predictions <- compute(nn_model, x_val_norm)$net.result

# Manually calculate RMSE for validation set
validation_rmse <- sqrt(mean((y_val - validation_predictions)^2))

# Predict using test data
test_predictions <- compute(nn_model, x_test_norm)$net.result

# Manually calculate RMSE for test set
test_rmse <- sqrt(mean((y_test - test_predictions)^2))

# Print RMSE values
cat("Validation RMSE for 2 hidden layers with 4 nodes each:", validation_rmse, "\n")
cat("Test RMSE for 2 hidden layers with 4 nodes each:", test_rmse, "\n")

# Plot the neural network
plot(nn_model, rep = "best")

```